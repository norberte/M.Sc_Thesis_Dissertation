\chapter{Methodology}

\section{\label{data_collection}Data Collection}

    When mining GitHub and Stack Overflow, a natural starting point is to use GHTorrent \cite{gousios2013ghtorent} and SOTorrent \cite{baltes2018sotorrent} as data sources.
    \subsection{Stack Overflow data}
    
        SOTorrent\footnote{\label{SOTorrent}\url{https://empirical-software.engineering/projects/sotorrent/}} is an open data set based on periodic, official Stack Overflow data dumps. SOTorrent was built for the purposes of mining and analyzing the evolution of Stack Overflow posts. A new version of this data set based on the latest Stack Overflow data dump is released every 3 months. Each version of the SOTorrent datas set can be downloaded from Zenodo\footnote{\label{SOTorrent_Zenodo} \url{https://zenodo.org/record/2273117}}. The latest versions of the SOTorrent data set are also available online, and can be queried through Google BigQuery\footnote{\label{BigQuery} \url{https://bigquery.cloud.google.com/dataset/sotorrent-org:2018_12_09}}.
        
        When downloading the SOTorrent data set, the version from \textit{December 9th 2018} was used, as it was the latest version available at the start of the project. The SOTorrent data set has its files organized into tables. Not all tables were downloaded, as certain tables related to post evolution (e.g. PostHistory, PostVersion tables) were deemed to be unrelated to the expertise learning task. The tables downloaded included Users, Posts, PostLinks, PostType, PostReferenceGH, Comments, CommentUrl, GHMatches, Tags, Badges and Votes, totaling 29 GB of compressed raw data. After downloading and un-compressing all relevant data files, the data set contained just shy of 100 GB of data.
        
    \subsection{GitHub data}
    
        GHTorrent\footnote{\label{GHTOrrent}\url{http://ghtorrent.org/}} is also an open data set, which mirrors the data offered through the GitHub REST API\footnote{\url{https://developer.GitHub.com/}}. GHTorrent has been collecting data from all public projects available on GitHub and releases a new version of MySQL data dumps every month, while it also offers daily data dumps through MongoDB. Each version of MySQL data can be downloaded from GHTorrent\footnote{\label{GH_dowload}\url{http://ghtorrent.org/downloads.html}}. GHTorrent data can also be looked at and queried online, as it is available as a DBLite web interface\footnote{\label{GH_query}\url{http://ghtorrent.org/dblite/}}. 
            
        When downloading the GHTorrent data set, the version from \textit{March 1st 2019} was used, as it was the latest version available at the time of download. The data set has its files organized into large tables stored in compressed CSV files, totaling 96557 MB of compressed data. All files were downloaded and uncompressed which resulted in 21 raw data tables totaling over 400 GB of data.

\section{Data Storage}

    \subsection{Database Setup and Data Import}
        
        A MySQL database was the obvious choice of database, as both data sources come with SQL scripts performing table creations, data imports and general database manipulations\footnote{\label{SO_sql} \url{https://GitHub.com/sotorrent/db-scripts/tree/master/sotorrent}} \footnote{\label{GH_sql} \url{https://GitHub.com/gousiosg/GitHub-mirror/tree/master/sql}}. Managing disk space during the data import phase represented a challenge, as the raw, uncompressed data files totaled up to over 500 GB. Importing of the data into the database was not achieved all at once, but rather through several iterations of uncompressing only a few data files. After a file was uncompressed and the table was successfully imported into the database, its raw CSV or XML file was deleted to free up disk space. 
        
        During the data import phase it was discovered that using \textit{MyISAM} over \textit{InnoDB} as the database engine is more favorable, since there was a significant execution time difference between the two database engines when importing large amounts of data into the MySQL database. \textit{MyISAM} database engine does not support foreign keys constraints, while it supports table-level locking. On the other hand \textit{InnoDB} supports foreign key constraints and row-level locking. \textit{MyISAM} is preferred when performing tasks that require fast import and querying speed, while \textit{InnoDB} being the default database engine for MySQL is more optimal for regular operations. For this particular reason all of the large raw data files were imported into the database using the \textit{MyISAM} database engine. After all necessary database manipulations have been performed, the database engine was changed back to the default engine, \textit{InnoDB}, to allow foreign key constraints to be enforced. 
     
        \todo{Add table contains descriptions for each relation's content}
      
        \begin{figure}[!ht]
          \centering
          \includegraphics[width=0.9\textwidth]{SO_DB_schema.jpg}\\
          \caption{Database schema for Stack Overflow data.}
          \label{fig:so_schema}
        \end{figure}
        
 
        \begin{figure}[!ht]
          \centering
          \includegraphics[width=\textwidth]{GH_DB_schema.jpg}\\
          \caption{Database schema for GitHub data.}
          \label{fig:gh_schema}
        \end{figure}
        
        \todo{Re-do GitHub DB schema}
        
        Figure \ref{fig:so_schema} and \ref{fig:gh_schema} show the database schema and all attributes within each table of the SOTorrent and respectively the GHTorrent data set. All tables from GHTorrent are shown in figure \ref{fig:gh_schema}, while only the relevant tables from SOTorrent are shown in figure \ref{fig:so_schema}. 


    \subsection{Linking together GHTorrent and SOTorrent}\label{Linking_SO_GH}
    
        In order to perform cross platform analysis of GitHub and StackOverflow, the linking of GHTorrent and SOTorrent datasets is needed. This task requires identifying the same user's accounts on both platforms. Looking at relevant literature on this this problem, one can identify the task of \textit{identity merging}, which consists of identifying the same person in two or more different environments. Vasilescu et al. \cite{vasilescu2013stackoverflow} researched this problem rigorously and after careful consideration of limiting the number of false positives they made use of email addresses. In 2013, at the time of publishing their work, email addresses were present in the GitHub dataset, while in the StackOverflow dataset email addresses were hidden, but their MD5 hashes were available. Vasilescu et al. \cite{vasilescu2013stackoverflow} decided to ``merge (i.e., link) a GitHub and a Stack Overflow user if the computed MD5 hash [of the email in GitHub] is identical to the MD5 email hash [of Stack Overflow users]". This resulted in 93,771 GitHub users being linked to StackOverflow. Vasilescu et al. \cite{vasilescu2013stackoverflow} further investigated, and came to the conclusion that out of 93,771 linked users only 46,967 users were active at the time. The linked data set alongside a replication package has been made public by the authors of the paper\footnote{\label{bodgan_dataset}\url{https://www.win.tue.nl/mdse/stackoverflow/}}. This data set has been used multiples times to analyze the interaction of GitHub and Stack Overflow data \cite{badashian2014involvement} and \cite{lee2017GitHub}, thus it became the backbone of the data sampling process behind linking together GitHub and Stack Overflow users.  

        There was one major problem with using Vasilescu et al. \cite{vasilescu2013stackoverflow}'s data set. The replication package contained a data set linking GitHub email addresses to Stack Overflow user IDs, without fully mapping GitHub user IDs to Stack Overflow user IDs, therefore the mapping of GitHub email addresses to user IDs was needed. This mapping was possible when Vasilescu et al. \cite{vasilescu2013stackoverflow} published their work, but after March 2016 GHTorrent was not allowed to store email addresses in their open data sets, as it violated GDPR compliances. The solution to this problem was to download the \textit{User} table data from GHTorrent's last available version that still contains email addresses of GitHub users in the data set. This older version of GHTorrent is dated \textit{February 1st 2016}, and it was downloaded, then imported into the MySQL database. A manual check was performed on matching user IDs and login names between the \textit{User} table's March 2019 and February 2016 versions. This manual check made sure that the user IDs from both versions of the table are referring to the same login name. If this was not the case, the user was dropped from the data set, due to inconsistency of the two versions of table. This manual check assured consistency in the data linkage between Stack Overflow and GitHub users, but resulted in the removal of over 10,000 users from the original 93,771 users linked. The final data set contains 83,550 user accounts being linked between the two platforms, offering a unique GitHub user ID to Stack Overflow user ID mapping. It can be speculated that removed users either deleted or changed the login name of their GitHub account between 2016 and 2019, thus causing the inconsistency in the two versions of the \textit{User} table.
        
    \subsection{Discarding Unlinked Data}
    
       As mentioned in section \ref{data_collection}, the total size of the GHTorrent data set was over 400 GB of data, while SOTorrent contained close to 100 GB of data. In order to make data querying and processing more efficient, the reduction the full data set was desired. In order to perform analysis only on the linkage between GitHub and Stack Overflow, only successfully linked data is needed. Filtering out unlinked data was done by iterating through each existing table in the database and keeping only the observations that are connected or related to users with a user ID present in the list of 83,550 unique user IDs linked. Discarding unlinked data reduced the size of the data set and allowed only linked data to be analyzed. This process reduced the size of the MySQL database from over 500 GB of data to 122 GB.
        
\section{Data Aggregation} 

    The end goal of the analysis on Stack Overflow and GitHub data was to create topic models capable of deciphering the hidden patterns of underlying topics that represent a user's activity on a platform. Tian et al. \cite{tian2013predicting} modeled user topical interest and expertise by building LDA models on user activity (user profile) data. The authors have showed that a Stack Overflow user's expertise can be extracted using a topic model applied over a set of textual documents consisting of the user's activity on the platform collected into a user profile.

    In order to perform cross platform analysis of GitHub and Stack Overflow users' expertise, the modeling of their expertise through LDA models is needed. LDA topic models are fitted on a set of documents. In order to fit topic models on GitHub and StackOverflow user data, one needs to define what a document consists of, and how to extract a user's profile from a platform such as GitHub or Stack Overflow. This process will be explained in the next three subsections.
    
    \subsection{Extracting Stack Overflow User Profiles\label{SO_userProfileExtraction}}
    
        When deciding how to create documents for the LDA topic model, the simplest approach was to define a document as a single user profile. Each user present in the data set of 83,550 linked users described in section \ref{Linking_SO_GH} has one document (user profile) describing their activity on Stack Overflow. This document contains all of their activity on the platform. Extracting a user's activity (user profile) from all the public data stored about a user started by inspecting each table of the SOTorrent database schema \ref{fig:so_schema} for attributes that contain relevant and meaningful textual information about a user. The relevant textual attributes included into the user profiles were badge names obtained by a user, the description on the profile page's about me section, questions posted by a user, answers given by a user, titles and tags of posts that a user participated in, and comments made by a user to any post. 
          
        \begin{table}[!htbp]
            \centering
            \label{tab:SO_userProfileExtraction}
            \caption{Sample}
            \vspace{6pt} % Required to get proper spacing between caption and table
            \begin{tabular}{|p{3.3cm}|p{2.7cm}|p{8cm}|}
               \toprule
               \textbf{Attribute Name} & \textbf{Attribute(s)} & \textbf{Derivation} \\
               \toprule
                Badges & Badges.Name & Concatenation of list of badges obtained by the user \\
                About Me & Users.AboutMe & Stack Overflow user profile's about me description \\
                Post Answer & Posts.Body, AcceptedAnswerId & Take a user's each answer and concatenate it with the question that it is related to  \\
                Post Question & Posts.Body & Take a user's each question and concatenate it with the accepted answer it is related to  \\
                Title and Tags for Questions & Posts.Tags, Posts.Title & Concatenate the post tags and title for each question that the user asked \\
                Title and Tags for Answers & Posts.Title, Posts.Tags & Concatenate the post tags and title for each answer that the user provided \\
                Comments & Comments.Text, Posts.Body, Posts.Title & Take a user's each comment and concatenate it to the post (question or answer) it is related to \\
               \bottomrule
            \end{tabular}
        \end{table} 
        
        A detailed description on how each attribute from the user profiles was processed can be found in table \ref{tab:SO_userProfileExtraction}. For the \textit{Post Answer} attribute in order to restore the semantic context of the user's answer, the question being asked is concatenated before the answer, thus creating a question-answer pair. For the same reason the \textit{Post Question} attribute is getting the accepted answer concatenated in, thus adding more contextual clarity to the user's question. This process is being applied to the \textit{Comments} attribute as well, creating question-comment or answer-comment pairs, depending on what the comment is related to. At the end of this process all attributes in the user profile get merged into one large document representing a user's full activity on the platform. 
        
    \subsection{Extracting GitHub User Profiles\label{GH_userProfileExtraction}}
    
        Each user present in the data set of 83,550 linked users described in section \ref{Linking_SO_GH} has one document (user profile) describing their activity on GitHub. Extracting a user profile was done my inspecting each table of the GHTorrent database schema \ref{fig:gh_schema} for attributes that contain relevant textual information about a user. The textual attributes selected to be part of the user profile data include the names, labels, languages used and description of each repository that a user owns, also their commit and pull request comments posted on GitHub. 
        
        \begin{table}[!htbp]
            \centering
            \caption{Sample}
            \label{tab:GH_userProfileExtraction}
            \vspace{6pt} % Required to get proper spacing between caption and table
            \begin{tabular}{|p{3.3cm}|p{3cm}|p{7.7cm}|}
               \toprule
               \textbf{Attribute Name} & \textbf{Attribute(s)} & \textbf{Derivation} \\
               \toprule
                Project Name, Description \& Metadata & Projects.[name, description, language], Repo-Labels.name & Take each user's each project and concatenate the repository's name, description, languages used and repository labels it contains\\  
                Commit-Comments & Commit-Comments.body & Concatenate a user's list of commit comments \\
                Code-review Comments & Pull-Request-Comments.body & Concatenate a user's list of code review (pull request) comments \\
               \bottomrule
            \end{tabular}
        \end{table}
        
        A detailed description on how each attribute from the user profiles was processed can be found in table \ref{tab:GH_userProfileExtraction}. Attributes such as the name, description and labels of a repository, alongside the programming languages used are valuable information describing the content of a repository on GitHub. Discussion between users in the form of comments to commits and pull request comments are also essential to establish contextual details about the project, thus these attributes have been included in the user profile. All the above listed attributes get merged into one large document representing a user's full activity on the platform. 
        
    \subsection{Creating Time Based User Profiles\label{past_recent_full_segm}}
        After defining how to extract user profiles from both Stack Overflow and GitHub, it is worth factoring in a \textit{timeline} into the user profile. The simplest approach would be to model recent and past activity of users by splitting their activity into before and after a specific date. Being able to model past and recent user profile data separately creates opportunities for comparison analyses to be done on the evolution of expertise of GitHub and Stack Overflow users. A decision was made to define recent activity as the activity performed by a user in the last 3 years. 2019 being the year that this analysis has taken part, the split between past and recent activity was created by any user profile data being dated before or after \textit{January 1st 2016}.
        
        \todo{talk about full, past, and recent versions of the dataset}

\section{Data Cleaning}

    Textual data on Stack Overflow and GitHub contains a large variety of software engineering related domain specific wording mixed with source code. Stack Overflow posts contain rich software artifacts such as code snippets, text blocks explaining problems and solutions using text and source code, comments which can be associated with a question or an answer, and hyperlinks to references such as API documentation or an academic paper explaining an answer. In their study of GitHub data, Liao et al. \cite{liao2019status} mentions that developers on GitHub ``talk about project bugs, enhancements, and tasks in issue discussions". The authors stated that a text corpus built on GitHub data will most likely contain ``plenty of code, warnings, messages, and technical terminology in addition to more general natural language [...] which allow for easy detection of status and/or relevant expertise".
    
    One can conclude that textual data on both GitHub and Stack Overflow captures a variety of aspects of software development, but its technical terminologies, mix of text and source code makes it difficult to extract the proper textual data to be analyzed. When performing text pre-processing on both sources of data, one needs to find the right balance between more than a dozen pre-processing techniques in order to clean up the data just the right way, without removing software engineering domain specific words and damaging the contextual details of software artifacts. Deciding on a precise set of techniques for the pre-processing routine was a challenging task, thus extensive research was done on what text pre-processing techniques were previously used when analyzing GitHub and Stack Overflow data.
    
    Tian et al. \cite{tian2013predicting} pre-processed textual data by performing tokenization, stop-word removal and stemming. They also split function names from cleaned up source code related keywords in the text processing phase.
    
    Campbell et al. \cite{campbell2015latent} mentions in a book chapter on extracting topics from software engineering data that generally those who use LDA apply the following pre-processing steps: ``loading text, mapping text into final textual representation, [perform] lexical analysis of the text, optionally removing stop words, optionally stemming, building a vocabulary, optionally removing uncommon or very common words and mapping each text document into a word-bag".
    
    Treude and Wagner \cite{treude2019predicting} have performed similar, but separate pre-processing routines on StackOverflow and GitHub data. Their Stack Overflow data cleaning routine consisted of removing line breaks, code blocks, all HTML tags, replacing HTML symbols and strings indicating special characters with their corresponding character, and replacing sequences of whitespace with a single space. Their GitHub pre-processing routine is the same as the above Stack Overflow data cleaning routine, with a few extra steps, such as removing vertical and horizontal lines, code comments, characters denoting sections headers, characters that indicate formatting or links. 
    
    Efstathiou et al. \cite{efstathiou2018word} converted text from Stack Overflow into lowercase, removed code snippets and HTML tags, then performed a ``conservative punctuation removal, keeping symbols that often appear in programming commands, such as ‘+’ and ‘\#’ which are essential for differentiating ‘C’, ‘C++’, and ‘C\#’ from one another."
    
    Boyd-Graber et al. \cite{boyd2014care}'s book chapter recommends HTML symbol and  stopword removal, then normalizing strings by converting to lower case and applying any kind of stemming algorithm, then apply tokenization and any kind of phrase detection algorithm as the proper pre-processing routine before fitting topic models on textual data.
    
    Liao et al. \cite{liao2019status} analyzed issue discussions on GitHub, and in their pre-processing steps they filtered out code-specific language, which was enclosed in HTML tags. The authors then removed short posts contained less than five tokens and distinguished between data generated by developers who never committed code to a project, and those who did. 
    
    Based on the above data cleaning routines used when working with software engineering domanin specific textual data, a \emph{user level} and a \emph{corpus level} pre-processing routine was developed. 
    
    \subsection{User Level Text Processing}
    
        The user level pre-processing routine is executed on each user's aggregated textual data individually during the user profile extraction processing described in sections \ref{SO_userProfileExtraction} and \ref{GH_userProfileExtraction}. The user level routine involved the removal of html links, symbols and tags, proceeded by stop-word, mentions (using @) and multiple white space removal. Finally, a tokenization technique is applied, then individual tokens consisting of only numbers or punctuation with no words present in the token are removed. This data processing routine is executed on the already aggregated (concatenated) data obtained using the description in sections \ref{SO_userProfileExtraction} and \ref{GH_userProfileExtraction}.
        
    \subsection{Corpus Level Text Processing}
    
        The corpus level pre-processing routine is executed on each individual text corpus (GH-past, GH-recent, GH-full, SO-past, SO-recent and SO-full) before the model fitting, but after the data aggregation process. The corpus level pre-processing includes tokenization, standardization of tokens, frequent phrase detection, then removing tokens that include only numbers and punctuation(if any). A subset of symbols and punctuations were also removed, keeping symbols such as ‘+’, ‘\_’, ‘.’ and ‘\#’, which often appear in programming contexts. Finally, rare and very common tokens are removed by filter out tokens that occur in less than 10 documents, or more than 50\% of the documents. In this context standardization of tokens means re-writting common tokens spelled multiple ways as a unique token. Examples of such common tokens include ‘node.js’, ‘node-js’, ‘node\_js’ for ‘nodejs’; ‘angular.js’, ‘angular-js’, ‘angular\_js’ for ‘angularjs’; or ‘js’ for ‘javascript’, and many more. A list of the most frequent phrases detected in the GH-full and SO-full data sets can be found in appendix \ref{frequentExpressions}. These common phrases used in the software engineering domain have totally different meaning as a phrase(bigram), thus they have been re-written as a single word (unigram), using a ‘\_’ character connecting the phrases. As a final note on data cleaning, both user and corpus level pre-processing routines were designed based on common best practises of previous data cleaning routines (\cite{tian2013predicting}, \cite{campbell2015latent}, \cite{treude2019predicting}, \cite{efstathiou2018word}, \cite{boyd2014care}, \cite{liao2019status}).

\section{Topic Modeling}

% why are we using topic models ?
    The inspiration to use topic modeling to learn cross-platform developer expertise came from Tian et al. \cite{tian2013predicting}'s work. The authors used topic models to predict the best answerer for a new question on Stack Overflow. Their approach learnt user topical expertise and interest levels by profiling each user's previous activity and reputation on Stack Overflow. In software engineering applications when working with large collections of textual data the most popular topic modeling technique to use is Latent Dirichlet Allocation (LDA) \cite{campbell2015latent}. 
    
    In their book chapter titled \textit{Latent Dirichlet Allocation: Extracting Topics from Software Engineering Data}, Campbell et al. \cite{campbell2015latent} is stating that ``LDA can be used to summarize, cluster, link, and preprocess large collections of data because it produces a weighted list of topics for every document in a collection dependent on the properties of the whole. These lists can then be compared, counted, clustered, paired, or fed into more advanced algorithms. Furthermore, each topic is comprised of a weighted list of words which can be used in summaries."
    
    LDA models have been previously used to learn semantic similarity between user profiles, posts (\cite{tian2013predicting}) and even source codes (\cite{arwan2015source}) on Stack Overflow. The proposed novel approach is creating a cross-platform developer expertise extraction task consisting of topical expertise learnt from topic analysis on user profiles extracted through mining public software repository platforms like Stack Overflow and GitHub. Campbell et al. \cite{campbell2015latent}'s explanation on the use of LDA models shows that extracting such expertise is possible by providing a summary or ranking of the the weighted list of topic words for each topic that the user is present in or belongs to. 
        
    \subsection{Model Setup \label{activeUser_Def}}
    
        As described in section \ref{past_recent_full_segm}, there are three different versions of Stack Overflow and GitHub corpora: past, recent and full corpus. Separate topic models are fitted on GitHub and Stack Overflow corpora. Separate models are created for each one of the past, recent and full versions of the corpus, thus a total of six topic models are fitted, three on processed Stack Overflow data (named \emph{SO-past, SO-recent and SO-full}) and three on processed GitHub data (named \emph{GH-past, GH-recent and GH-full}). The dictionary of terms used to fit the topic models is the same, and consistent across all three versions of a corpus. Each corpus consists of a collection of documents, which are user profiles. There are 83,550 linked users in the data set, each user having only one document (user profile) describing their activity, thus there are always 83,550 documents in each version of a corpus. It is worth noting that some users may be inactive on one or both of the platforms, thus making their user profile be an empty document due to the lack of activity on the platform. In the original intersected version of GitHub and Stack Overflow data set Vasilescu et al. \cite{vasilescu2013stackoverflow} defined an active user as someone who authored at least one commit, asked or answered a question between a specific period of time. In this project the definition of an active user is very similar to the one above. An active Stack Overflow user is defined as a user who posted a question or answer within a specified time range (past or recent time segmentation). An active GitHub user is defined as a user who performed a commit or a pull request within a specified time range (past or recent time segmentation).
    
    \subsection{Parameter Selection and Model Optimization}
    
        When analyzing the misuse of LDA common practises Agrawal et al. \cite{agrawal2018wrong} concluded that ``using the default settings of LDA for software engineering data can lead to systematic errors due to topic modeling instability". For instance, most LDA implementations recommend and use by default a fixed normalized asymmetric prior of $1.0 / topic-number$ for the $\alpha$ parameter, and a fixed prior of $0.01$ or $0.1$ for the $\beta$ parameter. In their study of good configurations of LDA models for GitHub and Stack Overflow data Treude and Wagner \cite{treude2019predicting} confirmed the findings of \cite{agrawal2018wrong} by concluding that ``popular rules of thumb for topic modelling parameter configuration are not applicable to textual corpora from GitHub and Stack Overflow. These corpora have different characteristics and require different configurations to achieve good model fit." Furthermore, Agrawal et al. \cite{agrawal2018wrong} clarifies by stating that ``any study that shows the topics learned from LDA, and uses them to make a particular conclusion, needs to first tune LDA,  [since] topics learned from untuned LDA are unstable, [but] after tuning, stability can be greatly increased." The authors also note that reusing hyper-parameter tunings suggested by other researchers from other data sets is not a reliable option, as the best LDA tunings differ from data set to data set, and LDA models always need to be re-tuned for all new data.
        
        To address these concerns of LDA misuse, the models fitted in this project use 400 iterations through the corpus when inferring the topic distribution of a corpus (instead of default 50 iterations). This should assure model's stability, as model convergence is checked before finishing the fitting process. For the random state of the model training initialization a fixed seed is used to promote reproducibility. 
        
        In terms of the $\alpha$ and $\beta$, hyper-parameter optimization is performed by combining the best practises of Bangash et al. \cite{bangash2019developers} and Treude and Wagner \cite{treude2019predicting}' work on finding the right hyper-parameters. Hoffman et al. \cite{hoffman2010online}'s implementation of \textit{Online learning for LDA models} is used to learn an asymmetric prior from the data for both $\alpha$ and $\beta$  hyper-parameters.
        
        Lastly, deciding on $k$, the number of topics present in the corpus is needed. The task of picking the right number of topics could be compared to the task of picking how many clusters to consider, when performing cluster analysis: there are many heuristics, but no general consensus on which one to use. For instance, Bangash et al. \cite{bangash2019developers} tried two different number of topics (K=20 and K=50) for their experimental runs, then picked whichever model Mallet’s optimizer \footnote{Mallet Optimizer explained: \url{https://dragonfly.hypotheses.org/1051}} returned topics that seemed close to actual topics. Panichella et al. \cite{panichella2013effectively} designed \textit{LDA-GA}, a genetic algorithm for parameter selection for LDA. Treude and Wagner \cite{treude2019predicting} defined the number of topics, $k$ as parameter range between 3 and 1000, then ran hyper-parameter optimization against this search space by perplexity as an evaluation metric. Hindle et al. \cite{hindle2012relating} chose the model with $k$ number of topics where the topics extracted were distinct enough. Furthermore, on top of these general rules there are many heuristics for deciding $k$ (\cite{arun2010finding}, \cite{cao2009density}, \cite{deveaud2014accurate}, \cite{griffiths2004finding}, \cite{zhao2015heuristic}).
        
        When deciding on $k$, the combination of Bangash et al. \cite{bangash2019developers} and Treude and Wagner \cite{treude2019predicting}'s methodology was followed. This meant that a parameter search space of [3, 100] was defined, then hyper-parameter optimization had been performed against this search space, with the an evaluation metric chosen in section \ref{evaluationMetric}. The upper bound of 100 topics was chosen based on the assumption that more than 100 topics would be difficult to label, interpret and keep track of. The parameters $\alpha$ and $\beta$ are being learnt during model fitting, thus  hyper-parameter optimization can be performed only on number of topics $k$ parameter, which was chosen based on whichever model maximized the evaluation metric. Optionally the $\beta$ parameter could be included in the hyper-parameter optimization process by defining a commonly used range of values, [0.001, 1] as search space.
    
    \subsection{Evaluation of Topic Models\label{evaluationMetric}}
    
        Evaluating LDA models is a highly debated topic in the academic community. There a few general rules to follow, but which ones to follow and to what extend is highly subjective. Boyd-Graber et al. \cite{boyd2014care} writes in their book chapter titled \textit{Care and Feeding of Topic Models: Problems, Diagnostics, and Improvements} that ``most evaluation of topic models has focused on statistical measures of perplexity or likelihood of test data. But this type of evaluation has limitations. The perplexity measure does not reflect the semantic coherence of individual topics learned by a topic model, nor does perplexity necessarily indicate how well a topic model will perform in some end-user task". Chang et al. \cite{chang2009reading} backs up these claims by suggesting that ``human judgments can be contrary to perplexity measures." 
        
        Treude and Wagner \cite{treude2019predicting} used perplexity for model evaluation, but they mention that perplexity is not the only metric which can be used to evaluate topic models, and in their future work they suggest using conciseness or coherence. Boyd-Graber et al. \cite{boyd2014care} also suggest the use of topic coherence as an evaluation metric. R{\"o}der et al. \cite{roder2015exploring} in their work titled \textit{Exploring the Space of Topic Coherence Measures} proposed a framework that allows the construction of ``existing word based coherence measures as well as new ones by combining elementary components". R{\"o}der et al. \cite{roder2015exploring}'s topic coherence measures seem to be the current state of the art in topic coherence advancements, thus their most promising coherence measures are used as evaluation metrics. The final model was selected based on a majority agreement of the highest coherence scores between all four coherence measures used (\emph{C\_v, C\_umass, C\_npmi and C\_uci}).
        
        Bangash et al. \cite{bangash2019developers} states that ``assigning names to topics is important as LDA does not label them, and carrying out analysis on lists of words is difficult". After evaluating and picking the best model based on the above criteria, the topic labeling methodology of Bangash et al. \cite{bangash2019developers} and Hindle et al. \cite{hindle2012relating} was followed, which consisted of assigning labels to the topics based on multiple authors’ consensus. When evaluating the topics of an LDA model, it is worth noting that Hindle et al. \cite{hindle2012relating} mentioned that ``labeling some topics might not be possible, [...] as some LDA topics may not have a valid interpretation", thus some topics need to be labeled under broad terms, or be discarded.

    
    \section{Expertise Ground Truth Survey}
    
        Bangash et al. \cite{bangash2019developers} performed a manual exploration of topics to validate if the LDA model's suggested topics are useful. They manually read randomly sampled documents and verified if the LDA model ``predicts the right topic for the [document], considering [their] understanding of the topics as the ground truths'' \cite{bangash2019developers}. This method of manual validation can be considered highly subjective, thus a more objective, and quantitative process would be desired. After careful consideration a developer expertise survey collecting human annotations to obtain a ground truth was a better way to understand the ground truth, thus an expertise survey was designed. 
    
        \subsection{Survey Setup}
        
            Using the data set built in section \ref{Linking_SO_GH} and based on the definition of an active user in section \ref{activeUser_Def} a random sample of 100 active users on both GitHub and Stack Overflow was taken. The list of profile page URLs of 100 random Stack Overflow and GitHub users can be found in the appendix, section \ref{surveyAppendix}. A visual of a sample survey can be seen in appendix, figure \ref{fig:sampleSurvey}. The list of 100 user profiles was split into lists of 10 user profiles, as the labeling of 100 user profiles would be too much to ask from the survey participants. Each list of 10 user profiles became part of an expertise survey, each containing 10 non-overlapping Stack Overflow and GitHub user profile URLs. It is worth nothing that the surveys were intentionally designed to not contain overlapping GitHub and Stack Overflow user profiles. This was done to ensure that a survey participant's annotations of a Stack Overflow profile would not be influenced by the same user's GitHub profile.
            
            The survey contained a list of 10 Stack Overflow and 10 GitHub user profile URLs, and the following instruction: ``Enter \emph{20} comma-separated words describing each user's expertise. Your answers needs to come from evaluating a user's full activity (i.e. every publicly available data that you can see and click-through) on GitHub or Stack Overflow''. Additionally the following sample answer for a fictional user was given: \emph{"pytorch, CNN, RNN, auto-encoders, Keras, git, tensorflow, python, java, C\#, web\_dev, machine\_learning, random\_forest, SVM, NLP, Java\_streams, distributed\_computing, parallel\_computing, R, statistics, visualization"}. This survey was given to computer science graduate students taking the 'Mining Software Repositories' graduate course taught by Professor Olga Baysal during the Fall 2019 semester at Carleton University. Participants to this expertise survey were rewarded by participation bonus marks by the instructor of the course. The class contained just shy of 30 graduate students. Each of one the 10 surveys was distributed to at least 2 students, aiming to get 20 participants creating 2 separate ground truth human annotations for each one of the 10 surveys. This goal was accomplished after giving the participants several weeks to complete an expertise survey. The survey asked for the participant's name in order to double check with them their annotations, if any word is unclear. After exporting the annotations of all participants, an expertise ground truth data set was created, and the names of participants were removed to complete the anonymity requirement. 
        
        \subsection{Processing of Human Annotations}
        
            The human annotations obtained from the survey participants were pre-processed using the same text processing routines used to clean the entire data set. Problems arose with misspelled words, and expressions or phrases that do not exist in the dictionary of the data set. To avoid inconsistencies between the corpora and human annotations manual checks were done on all annotations to correct misspelled words, remove duplicate annotations, remove annotations that made no sense, and re-write phrases out of vocabulary as individual words. 
            
            One final concern related to human annotations was unfortunate event of not every participant providing exactly the required amount of 20 keywords per user user. The intention was to represent the expertise of a developer with the minimum amount of words extracted from a topic. Hindle et al. \cite{hindle2012relating} recommends using a list of 20 words when describing an LDA topic, thus if a user's expertise belongs to only one topic there should be a minimum of 20 words describing the user's expertise. By not all participants not entering exactly 20 labels for a user's expertise, the length of the human annotations were inconsistent, which represented a problem when trying to measure annotator inter-agreement scores. Cohen's Kappa is a statistic that could be used to measure inter-rater agreement between annotations. Cohen's Kappa requires the annotations to be the same length, thus it could not be calculated for this ground truth data set. Instead, Krippendorff's alpha coefficient was considered to be computed in order to obtain an inter-rater agreement statistic. The last debate related to human annotations was about the aggregation of human annotations. When comparing an algorithm's prediction of expertise against the ground truth one needs to be able to easily calculate one or more metrics to determine the accuracy of the the algorithm. Having two annotations is inconvenient, as two separate accuracy values will be obtained, then the averaging of the two values could be calculated in multiple ways (weighted average, micro or macro average). To avoid such situation, a decision was made to aggregate annotations via set union or set intersect, and obtain one annotation only. 
            \todo{Decide against either union or intersect, and provide a reason.}
        
        \subsection{Limitations of the Study}
            As mentioned above, there were a few concern with processing and aggregating the human annotations. Such concerns create limitations for the ground truth data set. Firstly, the support of only a handful of frequent phrases (found in section \ref{frequentExpressions}) represents a limitation, as full support of any phrases would be desired. Secondly, the inconsistencies in annotation length represents a limitation too, as evaluating such ground truth data set could be challenging. Thirdly, the aggregation of annotations considering either set union or intersect of two separate annotations could be a threat to validity. \todo{Double check this with Olga}
        
\section{Data Analysis and Algorithm Design}
    %%for each technique
        %% what it is, where do you use it
    
    \subsection{Extracting/ Determining Expertise}
    
        %% mean_BLEU_score, mean_Jaccard_sim, Mean_Cos_sim
        \subsubsection{Topic Distribution based Expertise}
        %%% (mention Lda2vec and ETM as alternatives)
        
        \subsubsection{User and Topic Embeddings}
        %%% Avg and Max LDA Emb
        %%% Avg and Max word2vec Emb
        
        %%% k-means, then thresholding
        
    \subsection{Cross-platform expertise}
    %% RQ2 - quantitative diff - distr. of overlap or Jaccard sim.
    \subsection{Cross-platform Knowledge Transfer}
    %% RQ3 - most frequent words used in both platforms
    \subsection{Expertise Evolution}
%% RQ4 - changed ratio or Jaccard dist.

\section{Software Tools Used}



    
