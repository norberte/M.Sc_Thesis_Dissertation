\chapter{Discussion\label{chap:discussion}}

    In this chapter we will be presenting the overall findings from results in Section \ref{sec:findings}, then we will offer the implications of this study in Section \ref{sec:implications}, and lastly potential threads to validity will be presented in Section \ref{sec:validity}.
            
    % Once expertise has been extracted, a follow up question will focus on comparing the newly discovered trends from Stack Overflow and GitHub.
    
    \section{Findings\label{sec:findings}}
        Chapter \ref{chap:methodology} and \ref{chap:result}, Methodology and Results have shown that it is possible to learn and extract the expertise of software developers through topic modeling and data analysis. The above mentioned chapters have shown how user expertise profiles can be constructed from user activity data on collaborative platforms, which then could be leveraged into developer expertise. Multiple novel, robust, data-driven techniques have been designed in order to successfully extract the contextual expertise GitHub and Stack Overflow users. Based on the results from Section \ref{sec:results_rq1} the overall best performing technique is \textbf{W2V\_AVG}, \emph{Expertise Extraction using Pre-trained Word2Vec based User and Topic Embeddings obtained by Average-pooling}. 
        
        Cross-platform developer expertise have been has been thoroughly studied as well. Our analysis conducted on the similarity of expertise profiles across two separate collaborative platforms will expended the software engineering community's understanding of developer expertise built across multiple platforms. The most surprising finding out of this whole study was that expertise developed by users on GitHub and Stack Overflow is mostly different, with only a small portion of the population having similar expertise profiles on both collaborative platforms. Furthermore, our cross-platform developer expertise study also focused on what are the similarities between the two platforms and what kind of skills are the most transferable from one platform to another. The software engineering community will benefit from knowing that \emph{source code, version control and web development} related skills are most transferable knowledge, and they can be learnt by using both Stack Overflow and GitHub. The cross-platform developer expertise study ended in trying to better understand the expertise evolution on Stack Overflow and GitHub. This study discovered that most of the analyzed Stack Overflow and GitHub population has largely changed its expertise over time. We are not able to say if the change is in the right direction, but it is likely that the analyzed Stack Overflow and GitHub population are improving their skills, gaining knowledge and building expertise, instead of diminishing their current expertise.
        
        Throughout the results of the \emph{Expertise Extraction} task one interesting discovery was made. Each technique applied to GitHub user profile data produces significantly higher \emph{Cosine Similarity} scores than when applied to Stack Overflow user profile data. For the GitHub data set most models output \emph{Cosine Similarity} scores between 0.70 and 0.80, while for the Stack Overflow data set the same similarity metric is between 0.50 and 0.60. This represents a notable difference in contextual semantic similarity, especially if one looks at the sample cosine similarity scores in Table \ref{tab:cos_sim}. The novel, data-driven techniques presented in Section \ref{sec:algo_design} do not have the capability to explain why the same technique applied to two different data sets results in a significant difference in performance. Each technique is built upon a trained LDA model, which is sensitive to the quality and even order of input data. Treude and Wagner \cite{treude2019predicting}'s study concluded that GitHub and Stack Overflow text corpora have different characteristics due to their software artifact related content and require careful model selection to achieve good model fit. One can safely assume that the proposed techniques depend on the cleanliness of training data and careful LDA model fitting. We are hypothesizing that the difference in performance on GitHub and Stack Overflow data sets comes from the differences in characteristics of the two data sets. More specifically, we believe that the Stack Overflow data contains messier, less clean textual data, as Stack Overflow posts tend to contain both textual description of a problem, and in most cases source code is present in the post. We also hypothesize that the blend of natural text and source code in Stack Overflow posts is not cleaned up properly by the text pre-processing routine from Section \ref{sec:data_cleaning}. Unfortunately, as a consequence code snippets or keywords referring to source code do not get filtered out properly, and this could cause the drop in contextual semantic similarity of the techniques when applied on the Stack Overflow data set.
        
        The differences in characteristic of the two data sets is backed up by Treude and Wagner \cite{treude2019predicting}, who have studied the characteristics of GitHub and Stack Overflow corpora. They noticed that GitHub textual data contains fewer stop words than Stack Overflow data, which they believe it indicates the difference of technical descriptions present in GitHub, contrary to Stack Overflow posts, which contain more casual discussions. Based on Treude and Wagner \cite{treude2019predicting}'s study one other distinguishing feature between GitHub and Stack Overflow textual data is the median number of unique words (not stop words) in a document. Surprisingly, documents from GitHub on average contain almost twice as many unique words per document. It is worth noting that Treude and Wagner defined a document from GitHub as a repository's README file, not as an aggregated user activity profile. Nonetheless, Treude and Wagner showed that GitHub and Stack Overflow text corpora have different characteristics, and this claim affects the fitting of LDA models, as in their study GitHub corpora contained significantly higher number of topics than Stack Overflow corpora. Coincidentally, this is the case in our study too, as the number of topics present in best performing GitHub and Stack Overflow models mentioned in Section \ref{topic_patterns} can corroborate this claim.
    
        Our LDA model configuration and hyper-parameter tuning can be compared to the results of Treude and Wagner \cite{treude2019predicting}'s study on good topic modeling configurations. Similarities include both studies performing hyper-parameter tuning with almost identical parameter search-space, both studies using similar text pre-processing routines and the above mentioned fact that GitHub data contained significantly higher number of topics than Stack Overflow data. The main difference between our work and Treude and Wagner's work is the choice of evaluation of good model fit. While Treude and Wagner used perplexity, our approach can be defined as a task based evaluation of model selection. Our work is focused on model interpretability and usefulness to software developers and other practitioners, thus we evaluate an LDA model based on how semantically similar expertise terms it can extract compared to human annotations. This type of task based model evaluation is more in line with Campbell et al. \cite{campbell2015latent}'s recommendation on the use of LDA models for software engineering tasks, which we agree on. It is our belief that an arbitrary evaluation metric such as perplexity would not be able to provide an accurate or proper model selection process for the task of expertise extraction. This is the reason why we chose to perform task based LDA model evaluation.

    \section{Implications\label{sec:implications}}
    
        Implications of this research are numerous, with a variety of applications. We have defined 4 main target audiences for this research work: 1.) recruiters and recruitment oriented services, 2.) project managers, 3.) Stack Overflow and GitHub users, and 4.) research scientists. The implications for each one of these target audiences will be explained below. 
        
        The first target audience group to consider is recruiters and recruitment oriented services. This research work has closed the gap between potential job candidates and their expertise by providing a data-driven alternative to simply reading the candidate's CV or resume. The implementation of our expertise extraction algorithm into a recruitment agency's internal system would allow our research to be used by recruiters for finding and hiring the employees with the desired expertise. Our recommendation for recruiters is to not limit themselves only to employment-oriented platforms such as \emph{LinkedIn}\footnote{\url{https://www.linkedin.com}} (which could contain self-reporting bias; see Section \ref{LinkedIn_flaws}), and instead consider expertise profiles obtained via data-driven approaches, which theoretically should be less biased and more reliable. The advantage of looking at the big picture via expertise profiles is that the top experts in a specific area can be identified, which is important to companies.
        
        The second target audience of this research work is software companies, and project managers work on task assignments. Project managers are often faced with the task of deciding who is the best developer to handle a certain bug fix, code review or pull request. A task at hand needs to be assigned to one or more developers in the company. The optimal task assignment would allow the developer(s) with most expertise and experience in the domain to complete the task at hand. To achieve such an optimized task assignment the implementation of expertise detection and recommendation algorithms would be needed. Our recommendation for this target audience is to consider integrating the task assignment system described above into their task management tools, as software companies could benefit from an advanced bug fixing or code review assignment system by increasing the effectiveness and efficiency of their work.
         
        Stack Overflow and GitHub users are the the third group of target audience, as the entire research work is about analyzing and better understanding their skills, progress made, behavior and learning process. The ultimate goal of this research work is to make developers aware of their expertise, how to gain more of it, and become an expert. After extensive analysis of \emph{cross-platform developer expertise} our suggestion is that using multiple collaborative platforms is the optimal path towards gaining more knowledge and becoming an expert, as cross-platform developer expertise tends to be more diverse, thus creating further opportunities for faster and more effective ways of learning by collaboration. 
        
        Lastly, this research work is highly relevant to other researchers, data scientists and research scientists in the field of empirical software engineering and software artifact related data mining. More specifically, the use of LDA models and its combination with deep learning algorithms and other machine learning techniques is novel and innovative. This research work is a great example of how to convert large amounts of unstructured data into valuable insight by combining elements of both statistical learning and deep learning. In novel techniques 2 and 3 from Section \ref{sec:LDAbased_technique} and \ref{word2vec_model} elements of statistical learning, deep learning, feature extraction, down-sampling and thresholding are combined to generate user and topic embeddings, which then can be leveraged into mapping of users belonging to one or more topics. My recommendation for this target audience of researchers and research scientists would be to consider using this type of ensemble algorithm design more in their research work, as it can be a valuable, innovative and effective way of combining powerful algorithms to create purely data driven approaches. Another recommendation for researchers and research scientists working with textual data would be to consider using LDA models for more tasks. Campbell et al. \cite{campbell2015latent} stated that LDA models can be used to ``summarize, cluster, link, and pre-process'' large amounts of unstructured data. Based on this claim the number of opportunities and use cases should be countless. Furthermore, LDA models have an advantage over other models by scaling very well even for enormous amounts of data, thus we would advise the researchers to consider using more topic modeling in their work.
        
    \section{Threads to validity\label{sec:validity}}
        
        
        % known problems of LDA topics
        % ranking of expertise terms
        % no perfect data cleaning 
        % limitations on number of human annotations
        % limitations of the expertise study
        % topic coherence not being perfect measure for past-recent model selection
        % lack of user activity, or very little, insignificant data might throw off the LDA model
        % out of dictionary words for W2V based techniques
        
        %% As mentioned earlier, we ran into several challenges with processing and aggregating the human annotations. These challenges and our choices are for the ground truth data set. Firstly, the support of only a handful of frequent phrases (found in Section~\ref{frequentExpressions}) represents a limitation, as full support of any phrases would be desired. Secondly, the inconsistencies in annotation length represents a limitation too, as evaluating such ground truth data set could be challenging. Thirdly, the aggregation of annotations considering either set union or intersect of two separate annotations could be a threat to validity.
        
        %\cite{boyd2014care} states that ''problems with topics could be: 
        %    \begin{enumerate}
        %        \item Contains too general and specific words
        %        \item Contains mixed and chained topics
        %        \item Identical topics
        %        \item Nonsensical topics
        %        \item Contains "stop-word-like" words
        %    \end{enumerate}
