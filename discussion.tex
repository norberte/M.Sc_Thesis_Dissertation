\chapter{Discussion\label{chap:discussion}}

  In this chapter, we present the overall findings in Section \ref{sec:findings}, offer the implications of this study in Section \ref{sec:implications}, and discuss potential threats to validity in Section \ref{sec:validity}.
    
    \section{Findings\label{sec:findings}}
        Chapter \ref{chap:methodology} and Chapter \ref{chap:result}, i.e., Methodology and Results, respectively, have shown that it is possible to learn and extract the expertise of software developers through topic modeling and data analysis. We have demonstrated how user expertise profiles can be constructed from user activity data on collaborative platforms, which then could be leveraged into developer expertise. Multiple novel, robust, data-driven techniques have been designed to successfully extract the contextual expertise GitHub and Stack Overflow users. Based on the results from Section \ref{sec:results_rq1}, the overall best performing technique is \textbf{W2V\_AVG}, \emph{Expertise Extraction using Pre-trained Word2Vec based User and Topic Embeddings obtained by Average-pooling}. 
        
        One important discovery is that for Experiment \emph{1A} and \emph{2A} performed on GitHub data the Word2Vec based models (\textbf{W2V\_AVG} and \textbf{W2V\_MAX}) outperform the LDA based models (\textbf{LDA\_AVG} and \textbf{LDA\_MAX}), but this is not necessary true for Experiment \emph{1B} and \emph{2B} performed on Stack Overflow data. This discovery is intuitive from one perspective: Word2Vec based models use higher dimensional user and topic embeddings than the LDA based models, thus they can learn more contextual details from the text. The counter-intuitive fact about this insight is that the Word2Vec model was trained on Stack Overflow data, but it performs better on Experiments \emph{1A} and \emph{2A}, which are based on GitHub user activity data, not Stack Overflow. 
        
        Cross-platform developer expertise has been thoroughly studied as well. Our analysis conducted on the similarity of expertise profiles across two separate collaborative platforms can enrich and better support the software engineering community's understanding of developer expertise built across multiple platforms. The most surprising finding, in our opinion, is that expertise developed by users on GitHub and Stack Overflow is mostly different, with only a small portion of the population having similar expertise profiles on both collaborative platforms. Furthermore, our cross-platform developer expertise study also focused on what are the similarities between the two platforms and what kind of skills are the most transferable from one platform to another. The software engineering community, as well as software developers, can benefit from knowing that \emph{source code, version control and web development} related skills are most transferable knowledge, and they can be learnt by using both Stack Overflow and GitHub. The cross-platform developer expertise study also focused on a better understanding of the expertise evolution on Stack Overflow and GitHub. This study discovered that most of the analyzed Stack Overflow and GitHub developers have largely changed their expertise over time by adapting to new APIs, frameworks and libraries. We are not able to answer if the change is in the right direction, but the analyzed Stack Overflow and GitHub population are likely to improve their skills, gaining knowledge and building expertise, instead of diminishing their current expertise.
        
        Throughout the results of the \emph{Expertise Extraction} task one interesting discovery was made. Each technique applied to GitHub user profile data produces significantly higher \emph{Cosine Similarity} scores than when applied to Stack Overflow user profile data. For the GitHub data set most models output \emph{Cosine Similarity} scores between 0.70 and 0.80, while for the Stack Overflow data set the same similarity metric is between 0.50 and 0.60. This represents a notable difference in contextual semantic similarity, especially if one looks at the sample cosine similarity scores in Table \ref{tab:cos_sim}. The novel, data-driven techniques presented in Section \ref{sec:algo_design} cannot explain why the same technique applied to two different data sets results in a significant difference in performance. Each technique is built upon a trained LDA model, which is sensitive to the quality and even order of input data. Treude and Wagner \cite{treude2019predicting}'s study concluded that GitHub and Stack Overflow text corpora have different characteristics due to their software artifact related content and require careful model selection to achieve a good model fit. One can safely assume that the proposed techniques depend on the cleanliness of training data and careful LDA model fitting. We are hypothesizing that the difference in performance on GitHub and Stack Overflow data sets comes from the differences in characteristics of the two data sets. More specifically, we believe that the Stack Overflow data contains messier, less clean textual data, as Stack Overflow posts tend to contain both a textual description of a problem and in most cases source code is present in the post. We also hypothesize that the blend of natural text and source code in Stack Overflow posts is not cleaned up properly by the text pre-processing routine from Section \ref{sec:data_cleaning}. Unfortunately, as a consequence code snippets or keywords referring to source code do not get filtered out properly, and this could cause the drop in the contextual semantic similarity of the techniques when applied on the Stack Overflow data set.
        
        The differences in characteristics of the two data sets are backed up by Treude and Wagner \cite{treude2019predicting}, who have studied the characteristics of GitHub and Stack Overflow corpora. They noticed that GitHub textual data contains fewer stop words than Stack Overflow data, which they believe indicates the difference of technical descriptions present in GitHub, contrary to Stack Overflow posts, which contain more casual discussions. Based on Treude and Wagner \cite{treude2019predicting}'s study one other distinguishing feature between GitHub and Stack Overflow textual data is the median number of unique words (not stop words) in a document. Surprisingly, documents from GitHub on average contain almost twice as many unique words per document. It is worth noting that Treude and Wagner defined a document from GitHub as a repository's README file, not as an aggregated user activity profile. Nonetheless, Treude and Wagner showed that GitHub and Stack Overflow text corpora have different characteristics, and this claim affects the fitting of LDA models, as in their study GitHub corpora contained a significantly higher number of topics than Stack Overflow corpora. Coincidentally, this is the case in our study too, as the number of topics present in best performing GitHub and Stack Overflow models mentioned in Section \ref{topic_patterns} can support this claim.
    
        Our LDA model configuration and hyper-parameter tuning can be compared to the results of Treude and Wagner \cite{treude2019predicting}'s study on good topic modeling configurations. Similarities include both studies performing hyper-parameter tuning with almost identical parameter search-space, both studies using similar text pre-processing routines and the above mentioned fact that GitHub data contained a significantly higher number of topics than Stack Overflow data. The main difference between our work and Treude and Wagner's work is the choice of evaluation of a good model fit. While Treude and Wagner used perplexity, our approach can be defined as a task-based evaluation of model selection. Our work is focused on model interpretability and usefulness to software developers and other practitioners, thus we evaluate an LDA model based on how semantically similar expertise terms it can extract compared to human annotations. This type of task-based model evaluation is more in line with Campbell et al. \cite{campbell2015latent}'s recommendation on the use of LDA models for software engineering tasks, which we agree on. We believe that an arbitrary evaluation metric such as perplexity would not be able to provide an accurate or proper model selection process for the task of expertise extraction. This is the reason why we chose to perform a task-based LDA model evaluation.

    \section{Implications\label{sec:implications}}
    
        Implications of this research are numerous, with a variety of applications. We have defined four main target audiences for this research work: 1) recruiters and recruitment oriented services, 2) project managers and software companies, 3) Stack Overflow and GitHub users, and 4) research scientists. The implications for each one of these target audiences are explained next. 
        
        \subsection{Recruiters and Recruitment Oriented Services}
        
            The first target audience group to consider is recruiters and recruitment oriented services. This research work has closed the gap between potential job candidates and their expertise by providing a data-driven alternative to simply reading the candidate's CV or resume. The implementation of our expertise extraction algorithm into a recruitment agency's internal system would allow our research to be used by recruiters for finding and hiring the talent with the desired expertise. 
            
            Our recommendation for recruiters is to not limit themselves only to employment-oriented platforms such as \emph{LinkedIn}\footnote{\url{https://www.linkedin.com}} (which could contain self-reporting bias; see Section \ref{LinkedIn_flaws}), and instead consider expertise profiles obtained via data-driven approaches, which theoretically should be less biased and more reliable. The advantage of looking at the big picture via expertise profiles is that the top experts in a specific area can be identified, which is important to companies.
            
        \subsection{Project Managers and Software Companies}
        
            The second target audience of this research work is software companies and project managers who work on task assignments. Project managers are often faced with the task of deciding who is the best developer to handle a certain bug fix, code review or pull request. A task at hand needs to be assigned to one or more developers in the company. The optimal task assignment would allow the developer(s) with most expertise and experience in the domain to complete the task at hand. To achieve such an optimized task assignment the implementation of expertise detection and recommendation algorithms would be needed. 
            
            Our recommendation for this target audience is to consider integrating the task assignment system described above into their task management tools, as software companies could benefit from an advanced bug fixing or code review assignment system by increasing the effectiveness and efficiency of their work.
        
        \subsection{Stack Overflow and GitHub Users}
        
            The third group of target audiences are Stack Overflow and GitHub users, as the entire research work is about analyzing and better understanding their skills, progress made, behaviour and learning process. The ultimate goal of this research work is to make developers aware of their expertise, how to gain more of it, and become an expert. 
            
            After extensive analysis of \emph{cross-platform developer expertise} we suggest that using multiple collaborative platforms is the optimal path towards gaining more knowledge and becoming an expert, as cross-platform developer expertise tends to be more diverse, thus creating further opportunities for faster and more effective ways of learning by collaboration. 
            
        \subsection{Researchers and Research Scientists}
        
            Lastly, this research work is highly relevant to other researchers, data scientists and research scientists in the field of empirical software engineering and software artifact related data mining. More specifically, the use of LDA models and its combination with deep learning algorithms and other machine learning techniques is novel and innovative. This research work is a great example of how to convert large amounts of unstructured data into valuable insight by combining elements of both statistical learning and deep learning. In novel Technique 2 and Technique 3 from Section \ref{sec:LDAbased_technique} and \ref{word2vec_model} elements of statistical learning, deep learning, feature extraction, down-sampling and thresholding are combined to generate user and topic embeddings, which then can be leveraged into the mapping of users belonging to one or more topics. 
            
            Our recommendation for researchers and research scientists would be to consider using this type of ensemble algorithm design more in their research work, as it can be a valuable, innovative and effective way of combining powerful algorithms to create purely data-driven approaches. Another recommendation for researchers and research scientists working with textual data would be to consider using LDA models for more tasks. Campbell et al. \cite{campbell2015latent} stated that LDA models can be used to ``summarize, cluster, link, and pre-process'' large amounts of unstructured data. Based on this claim the number of opportunities and use cases should be countless. Furthermore, LDA models have an advantage over other models by scaling very well even for enormous amounts of data, thus we would advise the researchers to consider using more topic modeling in their work.
        
    \section{Threats to Validity\label{sec:validity}}
    
        Our study is subject to several threats to validity which we address in this section.
        
        \textbf{Data pre-processing}:  First, as previously mentioned, the blend of natural text and source code in Stack Overflow posts causes some challenges to the text pre-processing routine from Section \ref{sec:data_cleaning}, thus not all code snippets are cleaned up and filtered out properly. One might say, why was not a better text pre-processing routine applied to Stack Overflow posts. As a fact, the pre-processing of Stack Overflow posts was re-done several times, each time improving the routine iteratively to filter out as much noise as possible. We converged to the best possible text pre-processing routine (see Section \ref{CorpusLevel_preprocessing}) which contains a few compromises concerning filtering out the noise associated with code snippets.
        
        \textbf{Known LDA limitations}: Boyd-Graber et al. \cite{boyd2014care} stated that there are a few problems with topics in LDA models. More specifically, they said it is likely that some topics will contain too general and specific words, or they might contain ``stop-word-like'' words. Other potential issues include mixed and chained topics, identical or nonsensical topics. Unfortunately, these risks exist when employing topic models. Our way of making sure that no such issues affected the results was to manually validate the topics generated by the best performing models and discard nonsensical topics. When it comes to the use of LDA models, we followed the best practices presented in Section \ref{LDA_bestPractises}, and carefully avoided all the pitfalls outlined in Section \ref{LDA_pitfalls}.
        
        \textbf{Use of relevance for ranking expertise terms}: The next threat to validity is related to the suitability of our topic word ranking metric commonly used in topic modeling called \emph{relevance}\cite{sievert2014ldavis}. We are aware that \emph{relevance} is not the only metric which can be used to rank words, but we chose this word ranking metric based on Sievert et al. \cite{sievert2014ldavis}'s convincing novel method for choosing which topic words allow the best interpretation of a topic. 
        
        \textbf{Expertise study limitations}: During the expertise study (see Section \ref{sec:expertise_survey}) we ran into several challenges with processing and aggregating the human annotations. These challenges created some limitations for the ground truth data set. Firstly, the support of only a handful of frequent phrases (found in Section~\ref{frequentExpressions}) represents a limitation as full support of any phrases would be desired. Secondly, the inconsistencies in annotation length represent a limitation too since evaluating such ground truth data set could be challenging. To address these annotation length inconsistencies we had no choice but to merge the two annotations into one final annotation using a set union, and this could be a threat to validity. Other limitations related to the expertise study include the relatively small number of ground truth annotations that we could collect. We are aware that labelling 100 out of 83,550 Stack Overflow and GitHub users' expertise is a relatively small ratio, but that is not the actual ratio. We wanted to select only active users to be part of our expertise study, thus we narrowed down our random selection to a population of 675 users who are currently active on both GitHub and Stack Overflow. From those 675 users, we randomly selected 100 users to be labelled by our human annotators, thus we argue that 100 users are a large enough sample, considering the circumstances related to user activity.
        
        \textbf{Model selection using topic coherence}: We chose the most promising coherence measures (\emph{C\_v, C\_umass, C\_npmi and C\_uci}) from R{\"o}der et al. \cite{roder2015exploring}'s work as evaluation metrics for four text corpora (GH-past, GH-recent, SO-past and SO-recent) explained in Section \ref{past_recent_full_segm}. One potential threat to validity could be that the above mentioned coherence measures are not the most suitable metrics for model selection. We are aware of other evaluation metrics, such as perplexity, and log-likelihood of held-out data set, but we believed that R{\"o}der et al.'s state-of-the-art topic coherence metrics are the best choice for model evaluation.
        
        \textbf{Data quality}: A potential threat to validity could be related to data quality, especially in the SO-recent data set. This data set lacks active users, as only less than 1,000 out of 83,550 users have a significant amount of user activity, and the rest are inactive users. The same issue applies to the LDA model fitted on the SO-recent data set, as lack of data could lead to misleading topic trends in SO-recent. This is the nature of the data set, thus we, unfortunately, can not mitigate this data quality issue on the SO-recent data set. However, we completely avoided using the past and recent data sets for the \emph{expertise extraction} task in the first research question, and used the SO-full and GH-full data sets for this task instead.
        
        \textbf{Unknown words}: The last potential threat to validity is related to the handling of out-of-dictionary words in Efstathiou et al. \cite{efstathiou2018word}'s pre-trained Word2Vec model. We make use of this pre-trained model in our third novel technique (see Section~\ref{word2vec_model}) to look up software artifact related contextually aware vector representation of words. When we encountered out-of-dictionary words, rather than assigning a random vector or a vector of zeros, we skipped the word and did not assign any vector representation to it, so it does not corrupt the average or max-pooling calculations executed on the matrix containing the pre-trained embeddings. \Edited{Fortunately, the percentage of unknown words with respect to the number of unique words in the dictionary is small. For the GitHub text corpus only 7.315 \% of the words, while for the Stack Overflow text corpus 9.649 \% of the words had no word2vec embeddings.}
        
        
